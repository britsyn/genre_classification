{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==1.8.0 torchaudio==0.8.0 numpy==1.20.0","metadata":{"id":"DPQ9427wjLeX","outputId":"52eb6a88-ebc1-4b1b-ecc7-e2c1a4fa82aa","execution":{"iopub.status.busy":"2023-04-21T18:41:26.437769Z","iopub.execute_input":"2023-04-21T18:41:26.438075Z","iopub.status.idle":"2023-04-21T18:42:28.175039Z","shell.execute_reply.started":"2023-04-21T18:41:26.438043Z","shell.execute_reply":"2023-04-21T18:42:28.174162Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torch==1.8.0\n  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n     |████████████████████████████████| 735.5 MB 10 kB/s              B 69.2 MB/s eta 0:00:03| 633.6 MB 72.9 MB/s eta 0:00:02██████████████████████████████ | 712.3 MB 68.4 MB/s eta 0:00:01\n\u001b[?25hCollecting torchaudio==0.8.0\n  Downloading torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n     |████████████████████████████████| 1.9 MB 54.7 MB/s            \n\u001b[?25hCollecting numpy==1.20.0\n  Downloading numpy-1.20.0-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n     |████████████████████████████████| 15.3 MB 45.3 MB/s            \n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (4.1.1)\nInstalling collected packages: numpy, torch, torchaudio\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.20.3\n    Uninstalling numpy-1.20.3:\n      Successfully uninstalled numpy-1.20.3\n  Attempting uninstall: torch\n    Found existing installation: torch 1.9.1\n    Uninstalling torch-1.9.1:\n      Successfully uninstalled torch-1.9.1\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 0.9.1\n    Uninstalling torchaudio-0.9.1:\n      Successfully uninstalled torchaudio-0.9.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nexplainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.6 requires google-cloud-bigquery-storage, which is not installed.\nthinc 8.0.15 requires typing-extensions<4.0.0.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\ntfx-bsl 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntfx-bsl 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.0 which is incompatible.\ntfx-bsl 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\ntfx-bsl 1.5.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<3,>=1.15.2, but you have tensorflow 2.6.2 which is incompatible.\ntensorflow 2.6.2 requires numpy~=1.19.2, but you have numpy 1.20.0 which is incompatible.\ntensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\ntensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow 2.6.2 requires wrapt~=1.12.1, but you have wrapt 1.13.3 which is incompatible.\ntensorflow-transform 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-transform 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.0 which is incompatible.\ntensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\ntensorflow-transform 1.5.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.2, but you have tensorflow 2.6.2 which is incompatible.\ntensorflow-serving-api 2.7.0 requires tensorflow<3,>=2.7.0, but you have tensorflow 2.6.2 which is incompatible.\nspacy 3.2.3 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.1 which is incompatible.\nfeaturetools 1.6.0 requires numpy>=1.21.0, but you have numpy 1.20.0 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\narviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\napache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\napache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.1 which is incompatible.\napache-beam 2.34.0 requires typing-extensions<4,>=3.7.0, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\nSuccessfully installed numpy-1.20.0 torch-1.8.0 torchaudio-0.8.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport torchaudio\nfrom IPython import display\nfrom IPython.display import clear_output\nfrom sklearn.metrics import confusion_matrix\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\n\n%matplotlib inline\n\nassert torch.__version__.startswith(\"1.8.0\")\nassert torchaudio.__version__ == \"0.8.0\"\n\ndevice=\"cuda:0\"","metadata":{"id":"37r09zfzFXgE","execution":{"iopub.status.busy":"2023-04-21T18:42:28.210993Z","iopub.status.idle":"2023-04-21T18:42:28.211616Z","shell.execute_reply.started":"2023-04-21T18:42:28.211390Z","shell.execute_reply":"2023-04-21T18:42:28.211415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Классификация аудиозаписей.","metadata":{"id":"Hm0T-WW8FXgF"}},{"cell_type":"markdown","source":"## Задание 1 (1 балл). Знакомство с данными.\n\n1. Скачайте датасет из [Google Drive](https://drive.google.com/file/d/12emmtpodmo1783e6VOOEjV20zAKl5dZR/view?usp=sharing) c и распакуйте в папку `./data`.\n\n2. Напишите `AudioDataset` класс, который будет принимать путь к файлам `train_part.csv` и `val_part.csv` и возращать тройки объектов `(x, y, len)`, где `x` - аудиозапись, `y` - класс аудиозаписи, `len` - длина аудиозаписи. Аудиозаписи **не должны постоянно храниться в RAM**, подгрузку _wav_ файлов надо сделать при запросе через `__getitem__` метод. Кроме того, надо сделать паддинг аудиозаписи - если она короче чем `pad_size` параметр, надо дополнять ее нулями.\n\n3. С помощью функции `display.Audio` проиграйте в ноутбуке пару аудиозаписей.\n","metadata":{"id":"CL4IbYsbH_EW"}},{"cell_type":"code","source":"# классы данных\nclasses = ['blues',\n           'classical',\n           'country',\n           'disco',\n           'hiphop',\n           'jazz',\n           'metal',\n           'pop',\n           'reggae',\n           'rock']","metadata":{"id":"kCsOPjrAgs62","execution":{"iopub.status.busy":"2023-04-21T18:42:41.465072Z","iopub.execute_input":"2023-04-21T18:42:41.465347Z","iopub.status.idle":"2023-04-21T18:42:41.472411Z","shell.execute_reply.started":"2023-04-21T18:42:41.465317Z","shell.execute_reply":"2023-04-21T18:42:41.471658Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(\n        self, \n        path_to_csv: str, \n        path_to_folder: str, \n        pad_size: int = 384000,\n        sr: int = 44100 # sample rate\n    ):\n        self.csv: pd.DataFrame = pd.read_csv(path_to_csv)[\"filename\"]\n            \n        self.path_to_folder = path_to_folder\n        self.pad_size = pad_size\n\n        self.sr = sr\n\n        self.class_to_idx = {classes[i]: i for i in range(10)}\n\n    def __getitem__(self, index: int):\n      # Напишите AudioDataset класс, который будет принимать путь к файлам\n      # train_part.csv и val_part.csv и возращать тройки\n      # объектов (x, y, len), где x - аудиозапись, y - класс\n      # аудиозаписи, len - длина аудиозаписи. Аудиозаписи не должны постоянно\n      # храниться в RAM, подгрузку wav файлов надо сделать при запросе\n      # через __getitem__ метод. Кроме того, надо сделать паддинг аудиозаписи \n      # - если она короче чем pad_size параметр, надо дополнять ее нулями.\n        ### YOUR CODE IS HERE ######\n\n        id, wav_class = self.csv.iloc[index]\n\n        path_to_wav = os.path.join(self.path_to_folder, f'{id}.wav')\n\n        wav, sr = torchaudio.load(path_to_wav)\n        wav = torch.flatten(wav)\n        length = len(wav)\n        if sr != self.sr:\n            resampler = torchaudio.transforms.Resample(sr, self.SAMPLE_RATE)\n            wav = resampler(wav)\n        if len(wav) < self.pad_size:\n          wav = torch.cat((wav, torch.zeros(self.pad_size - len(wav))))\n        wav = wav.squeeze()\n\n        return {'x' : wav,\n                'y' : self.class_to_idx[wav_class],\n                'len': length}\n        ### THE END OF YOUR CODE ###\n        \n    def __len__(self):\n        return self.csv.shape[0]","metadata":{"id":"lX3plyGFiUFV","execution":{"iopub.status.busy":"2023-04-21T18:43:31.965526Z","iopub.execute_input":"2023-04-21T18:43:31.965788Z","iopub.status.idle":"2023-04-21T18:43:31.977531Z","shell.execute_reply.started":"2023-04-21T18:43:31.965759Z","shell.execute_reply":"2023-04-21T18:43:31.976839Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# создадим датасеты\ntrain_dataset = AudioDataset(\"/kaggle/input/gtzan-dataset-music-genre-classification/Data/features_30_sec.csv\",\n                             \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\")\n# val_dataset = AudioDataset(\"../input/hw2iad/urbansound8k/val_part.csv\", \"../input/hw2iad/urbansound8k/data\")","metadata":{"id":"zleXZ2vBLy6l","execution":{"iopub.status.busy":"2023-04-21T18:45:04.597983Z","iopub.execute_input":"2023-04-21T18:45:04.598548Z","iopub.status.idle":"2023-04-21T18:45:04.696619Z","shell.execute_reply.started":"2023-04-21T18:45:04.598513Z","shell.execute_reply":"2023-04-21T18:45:04.695035Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/991469618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# создадим датасеты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataset = AudioDataset(\"/kaggle/input/gtzan-dataset-music-genre-classification/Data/features_30_sec.csv\",\n\u001b[0;32m----> 3\u001b[0;31m                              \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\")\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# val_dataset = AudioDataset(\"../input/hw2iad/urbansound8k/val_part.csv\", \"../input/hw2iad/urbansound8k/data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/4234143566.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_csv, path_to_folder, pad_size, sr)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m44100\u001b[0m \u001b[0;31m# sample rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     ):\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['ID', 'Class'], dtype='object')] are in the [columns]\""],"ename":"KeyError","evalue":"\"None of [Index(['ID', 'Class'], dtype='object')] are in the [columns]\"","output_type":"error"}]},{"cell_type":"code","source":"# проверим размеры датасетов\nassert len(train_dataset) == 4500\nassert len(val_dataset) == 935","metadata":{"id":"majqJhy0ix0C","execution":{"iopub.status.busy":"2022-04-01T19:58:03.032294Z","iopub.execute_input":"2022-04-01T19:58:03.032527Z","iopub.status.idle":"2022-04-01T19:58:03.036086Z","shell.execute_reply.started":"2022-04-01T19:58:03.0325Z","shell.execute_reply":"2022-04-01T19:58:03.035418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# проверим возращаемые значения __getitem__\nitem = train_dataset.__getitem__(0)\n\nassert item[\"x\"].shape == (384000, )\nassert item[\"y\"] == 0\nassert item[\"len\"] == 176400","metadata":{"id":"phsH9zRJjIT1","execution":{"iopub.status.busy":"2022-04-01T19:58:04.11032Z","iopub.execute_input":"2022-04-01T19:58:04.111079Z","iopub.status.idle":"2022-04-01T19:58:04.148082Z","shell.execute_reply.started":"2022-04-01T19:58:04.11101Z","shell.execute_reply":"2022-04-01T19:58:04.147086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = torch.tensor([[1, 2],\n                    [3, 4]])\nt.flatten().size()","metadata":{"id":"cMg0d3CcIU1l","outputId":"3b2215af-1783-4cb3-d704-611204a8c7d2","execution":{"iopub.status.busy":"2022-04-01T19:58:04.57146Z","iopub.execute_input":"2022-04-01T19:58:04.571863Z","iopub.status.idle":"2022-04-01T19:58:04.587938Z","shell.execute_reply.started":"2022-04-01T19:58:04.571822Z","shell.execute_reply":"2022-04-01T19:58:04.58731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# нарисуем и проиграем аудиозаписить\nitem = train_dataset.__getitem__(0)\nplt.figure(figsize=(16, 8))\nplt.plot(item[\"x\"])\n\ndisplay.Audio(item[\"x\"], rate=train_dataset.sr)","metadata":{"id":"E9dH-ErCpe0K","outputId":"ff7292c6-1b55-4ce6-b20c-fbed030051b9","execution":{"iopub.status.busy":"2022-04-01T19:58:05.433376Z","iopub.execute_input":"2022-04-01T19:58:05.433844Z","iopub.status.idle":"2022-04-01T19:58:08.515455Z","shell.execute_reply.started":"2022-04-01T19:58:05.433805Z","shell.execute_reply":"2022-04-01T19:58:08.51478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# создадим даталоадеры\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=32, \n    shuffle=True,\n    pin_memory=True, \n    drop_last=True,\n    num_workers=2\n)\nval_dataloader = DataLoader(\n    val_dataset, \n    batch_size=32,\n    pin_memory=True,\n    num_workers=2\n)","metadata":{"id":"7mePKZtWOcdW","outputId":"ee8053c9-d985-4fcb-aaf3-01021a8c7730","execution":{"iopub.status.busy":"2022-04-01T20:05:01.612413Z","iopub.execute_input":"2022-04-01T20:05:01.6129Z","iopub.status.idle":"2022-04-01T20:05:01.618275Z","shell.execute_reply.started":"2022-04-01T20:05:01.612859Z","shell.execute_reply":"2022-04-01T20:05:01.617495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 2. Рекуррентная сеть для классификации аудиозаписей по сырому сигналу (2 балла)\n\nПо своей сути аудиозапись является ни чем иным, как временным рядом - замеры микрофона делаются через равные промежутки времени и хранятся в виде последовательности. \n\nКак известно, рекуррентные сети отлично подходят для работы с различными последовательностями, в том числе и с временными рядами.\n\nОбучим простую реккурентную сеть для классификации аудиозаписей.\n\n1. Разбейте аудизаписить на окошки размером `1024` с шагом `256`. Для этих целей отлично подойдет метод `torch.Tensor.unfold`.\n1. Применим к каждому получившемуся окну аудиосигнала полносвязную сеть с активациями `ReLU` и  внутренними размерностям `(1024 -> 256 -> 64 -> 16)`.\n2. По получившимся последовательностям пройдемся двунаправленой (`bidirectional=True`) LSTM с двумя слоями (`layers=2`).\n3. Склеим c помощью `torch.cat` последние `hidden_state` для каждого слоя и применим к ним полносвязную сеть `(2 * hidden_size * num_layers -> 256 -> 10)` с активацией `ReLU`.\n\n![title](https://github.com/hse-ds/iad-applied-ds/blob/master/2022/hw/hw2/imgs/rnn_raw.png?raw=1)\n\n*Совет*: для убыстрения обучения имеет смысл в полносвязные сети добавить `BatchNorm`.","metadata":{"id":"lotD7Qf7kgYD"}},{"cell_type":"code","source":"class RecurrentRawAudioClassifier(nn.Module):\n    def __init__(\n        self, \n        num_classes=10,\n        window_length=1024,\n        hop_length=256,\n        hidden=256,\n        num_layers=2\n    ) -> None:\n        super().__init__()\n\n        self.window_length = window_length\n        self.hop_length = hop_length\n\n        ### YOUR CODE IS HERE ######\n        self.first_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.BatchNorm1d(1497, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.BatchNorm1d(1497, 64),\n            nn.ReLU(),\n            nn.Linear(64, 16),\n            nn.ReLU(),\n        )\n\n        self.rnn = nn.LSTM(input_size=16,\n                           hidden_size=hidden,\n                           num_layers=2,\n                           bidirectional=True,\n                           batch_first=True)\n\n        self.final_mlp = nn.Sequential(\n            nn.Linear(2 * 2 * hidden, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 16),\n            nn.ReLU(),\n            nn.LogSoftmax(),\n        )\n        ### THE END OF YOUR CODE ###\n\n    def forward(self, x, lens) -> torch.Tensor:\n        # разбейте сигнал на окна \n        # batch_windows.shape == (B, NUM WINDOWS, 1024)\n        batch_windows = x.unfold(-1, self.window_length, self.hop_length)\n\n\n        # примените к каждому окну полносвязную сеть\n        # batch_windows_features.shape == (B, NUM WINDOWS, 16)\n        batch_windows_features = self.first_mlp(batch_windows)\n\n        # примените к получившемся последовательностям LSTM и возьмите hidden state\n        output, (hidden_states, cell_state) = self.rnn(batch_windows_features)\n\n        # склейте hidden_state по слоям\n        # hidden_flattened.shape = (B, 2 * hidden_size * num_layers)\n        hidden_flattened = torch.flatten(torch.reshape(\n            hidden_states, (hidden_states.size(1), hidden_states.size(0), \n                            hidden_states.size(2))), start_dim=1, end_dim=2)\n\n        # примените полносвязную сеть и получим логиты классов\n        return self.final_mlp(hidden_flattened)","metadata":{"id":"oWASUw4LnY9n","execution":{"iopub.status.busy":"2022-04-01T19:58:09.113918Z","iopub.execute_input":"2022-04-01T19:58:09.114388Z","iopub.status.idle":"2022-04-01T19:58:09.129667Z","shell.execute_reply.started":"2022-04-01T19:58:09.114352Z","shell.execute_reply":"2022-04-01T19:58:09.128837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Обучим получившуюся модель.","metadata":{"id":"nhes4lBeqf8j"}},{"cell_type":"code","source":"def train_audio_clfr(\n    model, \n    optimizer, \n    train_dataloader, \n    sr,\n    criterion=torch.nn.CrossEntropyLoss(),\n    data_transform=None, \n    augmentation=None,\n    num_epochs=1, device='cuda:0',\n    verbose_num_iters=10\n):\n    model.train()\n    iter_i = 0\n\n    train_losses = []\n    train_accuracies = []\n\n    for epoch in range(num_epochs):  \n        for batch in train_dataloader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            lens = batch[\"len\"].to(device)\n\n            # применяем преобразование входных данных\n            if data_transform:\n                x, lens = data_transform(x, lens, device=device, sr=sr)\n\n            # примеменяем к логмелспектрограмме аугментацию\n            if augmentation:\n                x, lens = augmentation(x, lens)\n\n            probs = model(x, lens)\n            optimizer.zero_grad()\n            loss = criterion(probs, y)\n            loss.backward()\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n            # считаем точность предсказания\n            pred_cls = probs.argmax(dim=-1)\n            train_accuracies.append((pred_cls == y).float().mean().item())\n\n            iter_i += 1\n\n            # раз в verbose_num_iters визуализируем наши лоссы и семплы\n            if iter_i % verbose_num_iters == 0:\n                clear_output(wait=True)\n\n                print(f\"Epoch {epoch}\")\n\n                plt.figure(figsize=(10, 5))\n\n                plt.subplot(1, 2, 1)\n                plt.xlabel(\"Iteration\")\n                plt.ylabel(\"Train loss\")\n                plt.plot(np.arange(iter_i), train_losses)\n\n                plt.subplot(1, 2, 2)\n                plt.xlabel(\"Iteration\")\n                plt.ylabel(\"Train acc\")\n                plt.plot(np.arange(iter_i), train_accuracies)\n\n                plt.show()\n\n    model.eval()","metadata":{"id":"597FI7NglRXI","execution":{"iopub.status.busy":"2022-04-01T19:58:09.943707Z","iopub.execute_input":"2022-04-01T19:58:09.944196Z","iopub.status.idle":"2022-04-01T19:58:09.958851Z","shell.execute_reply.started":"2022-04-01T19:58:09.944148Z","shell.execute_reply":"2022-04-01T19:58:09.958093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# создадим объекты модели и оптимизатор\nrnn_raw = RecurrentRawAudioClassifier()\nrnn_raw.to(device)\noptim = torch.optim.Adam(rnn_raw.parameters(), lr=3e-4)","metadata":{"id":"taMJCqQyrB26","execution":{"iopub.status.busy":"2022-04-01T19:58:10.342792Z","iopub.execute_input":"2022-04-01T19:58:10.343545Z","iopub.status.idle":"2022-04-01T19:58:12.90684Z","shell.execute_reply.started":"2022-04-01T19:58:10.343501Z","shell.execute_reply":"2022-04-01T19:58:12.906007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучим модель\ntrain_audio_clfr(rnn_raw, optim, train_dataloader, train_dataset.sr)","metadata":{"id":"qp7KbnHHqygC","outputId":"c87f3ad8-1ad9-4899-9052-1db27b0aabfb","execution":{"iopub.status.busy":"2022-04-01T19:58:12.908385Z","iopub.execute_input":"2022-04-01T19:58:12.908771Z","iopub.status.idle":"2022-04-01T19:59:08.713356Z","shell.execute_reply.started":"2022-04-01T19:58:12.908736Z","shell.execute_reply":"2022-04-01T19:59:08.712536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посчитаем метрики на валидационном датасете.","metadata":{"id":"6SM4ZM68wj4g"}},{"cell_type":"code","source":"def plot_confusion_matrix(model, val_dataloader, sr, device, data_transform=None):\n    pred_true_pairs = []\n    for batch in val_dataloader:\n        x = batch[\"x\"].to(device)\n        y = batch[\"y\"].to(device)\n        lens = batch[\"len\"].to(device)\n\n        with torch.no_grad():\n            if data_transform:\n                x, lens = data_transform(x, lens, sr=sr, device=device)\n\n            probs = model(x, lens)\n\n            pred_cls = probs.argmax(dim=-1)\n\n        for pred, true in zip(pred_cls.cpu().detach().numpy(), y.cpu().numpy()):\n            pred_true_pairs.append((pred, true))\n\n    print(f\"Val accuracy: {np.mean([p[0] == p[1] for p in pred_true_pairs])}\")\n\n    cm_df = pd.DataFrame(\n        confusion_matrix(\n            [p[1] for p in pred_true_pairs], \n            [p[0] for p in pred_true_pairs], \n            normalize=\"true\"\n        ),\n        columns=classes, \n        index=classes\n    )\n    sn.heatmap(cm_df, annot=True)","metadata":{"id":"GSx9f4WwwsF6","execution":{"iopub.status.busy":"2022-04-01T19:59:08.719123Z","iopub.execute_input":"2022-04-01T19:59:08.719776Z","iopub.status.idle":"2022-04-01T19:59:08.731956Z","shell.execute_reply.started":"2022-04-01T19:59:08.719735Z","shell.execute_reply":"2022-04-01T19:59:08.731343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(rnn_raw, val_dataloader, train_dataset.sr, device)","metadata":{"id":"WFHiwGJwwyj8","execution":{"iopub.status.busy":"2022-04-01T19:59:08.734011Z","iopub.execute_input":"2022-04-01T19:59:08.735007Z","iopub.status.idle":"2022-04-01T19:59:15.995445Z","shell.execute_reply.started":"2022-04-01T19:59:08.734969Z","shell.execute_reply":"2022-04-01T19:59:15.994743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Вопрос* : Сильно ли отличается качество модели на тренировочной и валидационной выборке? Если да, то как думаете, в чем причина?\nв целом, они похожи!","metadata":{"id":"Uo_1cnkGzyNV"}},{"cell_type":"markdown","source":"## Задание 3. Построение Мел-cпектрограмм. (2 балла)\n\nСырой сигнал очень чувствителен ко многим факторам - увеличение/уменьшение громкости, внешние шумы, сменение тембра говорящего очень резко меняют сырой сигнал. Это влияет и на качество глубоких сетей, обученных на сыром аудиосигнале.\n\nДля построения надежных и устойчивых к переобучению моделей используют другое представление аудиоданных - спектрограммы, в том числе Мел-спектрограмму.\n\nИдея её построения заключается в следующем:\n1. Сигнал разбивается на временные интервалы (с пересечениями)\n2. К каждому временному интервалу применяется фильтр (как правило косинусоидальный)\n3. К профильтрованному сигналу применяется дискретное преобразование Фурье и вычисляются спектральные признаки сигнала.\n4. Спектральные признаки с помощью логарифмического преобразования приводятся в Мел-шкалу.\n\n![image](https://antkillerfarm.github.io/images/img2/Spectrogram_5.png)\n\nВ этом задании мы сами шаг за шагом напишем алгоритм построения мелспектрограммы и сравнимся с референсной функцией из `torchaudio`.","metadata":{"id":"5G8-l8yuRNLL"}},{"cell_type":"code","source":"from torchaudio.transforms import MelSpectrogram\n\n# референсная функця\ndef compute_log_melspectrogram_reference(\n    wav_batch, \n    lens,\n    sr,\n    device=\"cpu\"\n):\n    featurizer = MelSpectrogram(\n        sample_rate=sr,\n        n_fft=1024,\n        win_length=1024,\n        hop_length=256,\n        n_mels=64,\n        center=False,\n        ).to(device)\n\n    return torch.log(featurizer(wav_batch).clamp(1e-5)), lens // 256","metadata":{"id":"wubdQiRcGdOV","execution":{"iopub.status.busy":"2022-04-01T19:59:15.997411Z","iopub.execute_input":"2022-04-01T19:59:15.998161Z","iopub.status.idle":"2022-04-01T19:59:16.005299Z","shell.execute_reply.started":"2022-04-01T19:59:15.998121Z","shell.execute_reply":"2022-04-01T19:59:16.004411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# возьмем случайный батч\nfor batch in train_dataloader:\n    break\n\nwav_batch = batch[\"x\"]\nlens = batch[\"len\"]\n\n# посчитаем лог мелспектрограммы\nlog_melspect, lens = compute_log_melspectrogram_reference(wav_batch, lens, train_dataset.sr)\n\n# нарисуем получившиеся референсные значения\nfig, axes = plt.subplots(5, figsize=(16, 8))\n\nfor i in range(5):\n    axes[i].axis(\"off\")\n    axes[i].set_title(f\"Reference log melspectorgram {i}\")\n    axes[i].imshow(log_melspect[i].numpy())","metadata":{"id":"5xdUhnxxQIbT","execution":{"iopub.status.busy":"2022-04-01T19:59:16.00681Z","iopub.execute_input":"2022-04-01T19:59:16.007187Z","iopub.status.idle":"2022-04-01T19:59:18.216353Z","shell.execute_reply.started":"2022-04-01T19:59:16.007143Z","shell.execute_reply":"2022-04-01T19:59:18.213211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь сделаем то же самое сами. ","metadata":{"id":"y9NHPcQ9IL7-"}},{"cell_type":"code","source":"sr = train_dataset.sr\nn_fft=1024\nwin_length=1024\nhop_length=256\nn_mels=64","metadata":{"id":"SQackPL0JAlI","execution":{"iopub.status.busy":"2022-04-01T19:59:18.218181Z","iopub.execute_input":"2022-04-01T19:59:18.218732Z","iopub.status.idle":"2022-04-01T19:59:18.22313Z","shell.execute_reply.started":"2022-04-01T19:59:18.218675Z","shell.execute_reply":"2022-04-01T19:59:18.222109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nДля начала с помощью метода `unfold` разделим аудиосигнал на окна размера `win_lenght` через промежутки `hop_lenght`.","metadata":{"id":"S3QjWmr0JK6g"}},{"cell_type":"code","source":"windows = batch[\"x\"].unfold(-1, win_length, hop_length)\nassert windows.shape == (32, 1497, 1024)","metadata":{"id":"T2PhQ8MWQQOe","execution":{"iopub.status.busy":"2022-04-01T19:59:18.224507Z","iopub.execute_input":"2022-04-01T19:59:18.225017Z","iopub.status.idle":"2022-04-01T19:59:18.235197Z","shell.execute_reply.started":"2022-04-01T19:59:18.224982Z","shell.execute_reply":"2022-04-01T19:59:18.234002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нарисуем и проиграем сигнал из одного окна.","metadata":{"id":"VgDm4pdsJreZ"}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.plot(windows[0, 0])\n\ndisplay.Audio(windows[0, 0], rate=train_dataset.sr)","metadata":{"id":"rUF4iFkbQsqt","execution":{"iopub.status.busy":"2022-04-01T19:59:18.236801Z","iopub.execute_input":"2022-04-01T19:59:18.237177Z","iopub.status.idle":"2022-04-01T19:59:18.458545Z","shell.execute_reply.started":"2022-04-01T19:59:18.237143Z","shell.execute_reply":"2022-04-01T19:59:18.457814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь нам надо применить косинуисальный фильтр к сигналу из окна. Для этого с помощью `torch.hann_window` создадим косинусоидальный фильтр и умножим его поэлементно на все окна.","metadata":{"id":"A3rp6KtjKWmG"}},{"cell_type":"code","source":"filter = torch.hann_window(win_length)\nwindows_with_applied_filter = windows * filter[None, None, :]","metadata":{"id":"jTtg612XQ8NB","execution":{"iopub.status.busy":"2022-04-01T19:59:18.45997Z","iopub.execute_input":"2022-04-01T19:59:18.460524Z","iopub.status.idle":"2022-04-01T19:59:18.587482Z","shell.execute_reply.started":"2022-04-01T19:59:18.460483Z","shell.execute_reply":"2022-04-01T19:59:18.586689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.plot(windows_with_applied_filter[0, 0])\n\ndisplay.Audio(windows_with_applied_filter[0, 0], rate=train_dataset.sr)","metadata":{"id":"JsJCAGfeRKVM","execution":{"iopub.status.busy":"2022-04-01T19:59:18.592589Z","iopub.execute_input":"2022-04-01T19:59:18.592801Z","iopub.status.idle":"2022-04-01T19:59:18.918659Z","shell.execute_reply.started":"2022-04-01T19:59:18.592776Z","shell.execute_reply":"2022-04-01T19:59:18.917987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"С помощью `torch.fft.fft` примените дискретное преобразование фурье к каждому окну и возьмите первые `n_fft // 2 + 1` компоненты.\n\nДальше с помощью возведения элементов тензора в квадрат и `torch.abs()` получите магнитуды.","metadata":{"id":"191JHzI_K0KC"}},{"cell_type":"code","source":"fft_features = torch.fft.fft(windows_with_applied_filter)\nfft_magnitudes = torch.abs(fft_features ** 2)[:, :, :n_fft // 2 + 1]\nassert fft_magnitudes.shape == (32, 1497, 513)","metadata":{"id":"TfwpkUZcOflD","execution":{"iopub.status.busy":"2022-04-01T19:59:18.920086Z","iopub.execute_input":"2022-04-01T19:59:18.92059Z","iopub.status.idle":"2022-04-01T19:59:20.012631Z","shell.execute_reply.started":"2022-04-01T19:59:18.920556Z","shell.execute_reply":"2022-04-01T19:59:20.011864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Через `torchaudio.transforms.MelScale` создайте класс для перевода магнитуд в Мел-шкалу.","metadata":{"id":"pIvEu-3aMdS2"}},{"cell_type":"code","source":"melscale =  torchaudio.transforms.MelScale(n_mels=n_mels, sample_rate=sr, n_stft=n_fft // 2 + 1).to(device)","metadata":{"id":"PsHfVe4zOoYl","execution":{"iopub.status.busy":"2022-04-01T19:59:20.014191Z","iopub.execute_input":"2022-04-01T19:59:20.014448Z","iopub.status.idle":"2022-04-01T19:59:20.02096Z","shell.execute_reply.started":"2022-04-01T19:59:20.014412Z","shell.execute_reply":"2022-04-01T19:59:20.020253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нелинейное преобразование для перевода в Мел-шкалу выглядит следующим образом.","metadata":{"id":"tNUZz4m2NcIH"}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.axis(\"off\")\nplt.imshow(melscale.fb.cpu().numpy().transpose())","metadata":{"id":"T2nirK_MR9PM","execution":{"iopub.status.busy":"2022-04-01T19:59:20.0223Z","iopub.execute_input":"2022-04-01T19:59:20.022767Z","iopub.status.idle":"2022-04-01T19:59:20.108928Z","shell.execute_reply.started":"2022-04-01T19:59:20.022732Z","shell.execute_reply":"2022-04-01T19:59:20.108319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Примените Мел-шкалу к магнитудам.","metadata":{"id":"7YVcdanFNnqx"}},{"cell_type":"code","source":"mel_spectrogram = melscale(torch.transpose(fft_magnitudes, -1, -2).to(device)).to(device)\nassert mel_spectrogram.shape == (32, 64, 1497)","metadata":{"id":"SRo-H_r2SVA_","execution":{"iopub.status.busy":"2022-04-01T19:59:20.110277Z","iopub.execute_input":"2022-04-01T19:59:20.110736Z","iopub.status.idle":"2022-04-01T19:59:20.389955Z","shell.execute_reply.started":"2022-04-01T19:59:20.110701Z","shell.execute_reply":"2022-04-01T19:59:20.389167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сделайте обрезку значений по `1e-5` и примените `torch.log` для получения логарифмированной Мел-спектрограммы.","metadata":{"id":"jg9xL0GSORGw"}},{"cell_type":"code","source":"logmel_spectrogram = torch.log(mel_spectrogram.clamp(1e-5)).to(device)\nassert logmel_spectrogram.shape == (32, 64, 1497)","metadata":{"id":"5jTzCF3qSp1d","execution":{"iopub.status.busy":"2022-04-01T19:59:20.391134Z","iopub.execute_input":"2022-04-01T19:59:20.391393Z","iopub.status.idle":"2022-04-01T19:59:20.396796Z","shell.execute_reply.started":"2022-04-01T19:59:20.39136Z","shell.execute_reply":"2022-04-01T19:59:20.396059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Полученные логарифмированные Мел-Спектрограммы должны совпадать с референсными.","metadata":{"id":"Qpf_PuvSOzjK"}},{"cell_type":"code","source":"# нарисуем получившиеся значения\nfig, axes = plt.subplots(5, figsize=(16, 8))\n\nfor i in range(5):\n    axes[i].axis(\"off\")\n    axes[i].set_title(f\"Your log melspectorgram {i}\")\n    axes[i].imshow(logmel_spectrogram[i].cpu().numpy())","metadata":{"id":"lcdq-X8sSsd2","execution":{"iopub.status.busy":"2022-04-01T19:59:20.398065Z","iopub.execute_input":"2022-04-01T19:59:20.398739Z","iopub.status.idle":"2022-04-01T19:59:20.811458Z","shell.execute_reply.started":"2022-04-01T19:59:20.398705Z","shell.execute_reply":"2022-04-01T19:59:20.810797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь оформим эту логику в функцию.","metadata":{"id":"jY7egzCCPcw5"}},{"cell_type":"code","source":"# ваша реализация\ndef compute_log_melspectrogram(\n    wav_batch,\n    lens,\n    sr,\n    device=\"cpu\"\n):\n  ### YOUR CODE IS HERE ######\n  windows = wav_batch.unfold(-1, win_length, hop_length).to(device)\n  filter = torch.hann_window(win_length).to(device)\n  windows_with_applied_filter = windows * filter[None, None, :].to(device)\n  fft_features = torch.fft.fft(windows_with_applied_filter).to(device)\n  fft_magnitudes = torch.abs(fft_features ** 2)[:, :, :n_fft // 2 + 1].to(device)\n  melscale =  torchaudio.transforms.MelScale(n_mels=n_mels, sample_rate=sr, n_stft=n_fft // 2 + 1).to(device)\n  mel_spectrogram = melscale(torch.transpose(fft_magnitudes, -1, -2)).to(device)\n  logmel_spectrogram = torch.log(mel_spectrogram.clamp(1e-5)).to(device)\n  return logmel_spectrogram, lens // 256\n  ### THE END OF YOUR CODE ###","metadata":{"id":"re2zffcEPb2F","execution":{"iopub.status.busy":"2022-04-01T19:59:20.812637Z","iopub.execute_input":"2022-04-01T19:59:20.813334Z","iopub.status.idle":"2022-04-01T19:59:20.824296Z","shell.execute_reply.started":"2022-04-01T19:59:20.813297Z","shell.execute_reply":"2022-04-01T19:59:20.821552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Финальная проверка.","metadata":{"id":"cPCTsOiLRRPs"}},{"cell_type":"code","source":"assert torch.allclose(\n    compute_log_melspectrogram_reference(wav_batch, lens, train_dataset.sr)[0],\n    compute_log_melspectrogram(wav_batch, lens, train_dataset.sr)[0],\n    atol=1e-5\n)","metadata":{"id":"NZZj8q_ZQuHy","execution":{"iopub.status.busy":"2022-04-01T19:59:20.826874Z","iopub.execute_input":"2022-04-01T19:59:20.827755Z","iopub.status.idle":"2022-04-01T19:59:22.973719Z","shell.execute_reply.started":"2022-04-01T19:59:20.827717Z","shell.execute_reply":"2022-04-01T19:59:22.972968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 4. Рекуррентная сеть для классификации аудиозаписей по логарифмированным Мел-спектрограммам (1 балл)\n\nИзмените реализацию рекуррентной сети из задания 2, таким образом, чтобы она вместо сырого сигнала смогла принимать логарифмированные Мел-спетрограммы:\n1. Уберите шаги 1-2\n2. Сделайте вход LSTM равным 64\n\n![arch_mel](https://github.com/hse-ds/iad-applied-ds/blob/master/2022/hw/hw2/imgs/rnn_mel.png?raw=1)\n\n**За реализацию архитектуры дается 0.5 балла.**","metadata":{"id":"NN5KS-uufYDA"}},{"cell_type":"code","source":"class RecurrentMelSpectClassifier(nn.Module):\n    def __init__(\n        self, \n        num_classes=10,\n        window_length=1024,\n        hop_length=256,\n        hidden=256,\n        num_layers=2\n    ) -> None:\n        super().__init__()\n\n        self.window_length = window_length\n        self.hop_length = hop_length\n\n        ### YOUR CODE IS HERE ######\n        self.rnn = nn.LSTM(input_size=64,\n                           hidden_size=hidden,\n                           num_layers=2,\n                           bidirectional=True,\n                           batch_first=True)\n\n        self.final_mlp = nn.Sequential(\n            nn.Linear(2 * 2 * hidden, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 16),\n            nn.ReLU(),\n            nn.LogSoftmax(),\n        )\n        ### THE END OF YOUR CODE ###\n\n    def forward(self, x, lens):\n        ### YOUR CODE IS HERE ###### \n        # примените к получившемся последовательностям LSTM и возьмите hidden state\n        output, (hidden_states, cell_state) = self.rnn(torch.transpose(x, -1, -2))\n\n        # склейте hidden_state по слоям\n        # hidden_flattened.shape = (B, 2 * hidden_size * num_layers)\n        hidden_flattened = torch.flatten(torch.reshape(\n            hidden_states, (hidden_states.size(1), hidden_states.size(0), \n                            hidden_states.size(2))), start_dim=1, end_dim=2)\n\n        # примените полносвязную сеть и получим логиты классов\n        return self.final_mlp(hidden_flattened)\n        ### THE END OF YOUR CODE ###","metadata":{"id":"lrlBRTF-fV86","execution":{"iopub.status.busy":"2022-04-01T19:59:36.884559Z","iopub.execute_input":"2022-04-01T19:59:36.88482Z","iopub.status.idle":"2022-04-01T19:59:36.895719Z","shell.execute_reply.started":"2022-04-01T19:59:36.88479Z","shell.execute_reply":"2022-04-01T19:59:36.89497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_mel = RecurrentMelSpectClassifier()\nrnn_mel.to(device)\n\noptim = torch.optim.Adam(rnn_mel.parameters(), lr=3e-4)","metadata":{"id":"a9Z4a-W6jBQX","execution":{"iopub.status.busy":"2022-04-01T19:59:37.256191Z","iopub.execute_input":"2022-04-01T19:59:37.256711Z","iopub.status.idle":"2022-04-01T19:59:37.285004Z","shell.execute_reply.started":"2022-04-01T19:59:37.256668Z","shell.execute_reply":"2022-04-01T19:59:37.284359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_audio_clfr(rnn_mel, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram, num_epochs=10)","metadata":{"id":"at77cRtEjt-r","outputId":"bf535eb3-480a-4c50-cae5-6c5f5dbb7e66","execution":{"iopub.status.busy":"2022-04-01T20:00:44.121743Z","iopub.execute_input":"2022-04-01T20:00:44.122026Z","iopub.status.idle":"2022-04-01T20:04:04.896734Z","shell.execute_reply.started":"2022-04-01T20:00:44.121993Z","shell.execute_reply":"2022-04-01T20:04:04.895906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посчитаем метрики на валидационном датасете.","metadata":{"id":"dpl1QhKlVkot"}},{"cell_type":"markdown","source":"**Задание: для получения 0.5 балла сделайте подбор гиперпараметров и добейтесь accuracy модели выше 0.8 на валидационном датасете.**","metadata":{"id":"hOXx3pG58cHm"}},{"cell_type":"code","source":"plot_confusion_matrix(rnn_mel, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"id":"U2WPjo4tjyy0","outputId":"57097d94-ec60-4db9-8226-e0fb513f7064","execution":{"iopub.status.busy":"2022-04-01T20:04:10.640247Z","iopub.execute_input":"2022-04-01T20:04:10.64051Z","iopub.status.idle":"2022-04-01T20:04:16.976546Z","shell.execute_reply.started":"2022-04-01T20:04:10.640479Z","shell.execute_reply":"2022-04-01T20:04:16.975815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 5. Cверточная сеть для классификации аудиозаписей по мелспектрограммам. (2 балла)\n\nЛегко заметить, что мелспектрограммы имеют четко выраженные паттерны - если приноровиться, то даже человек, посмотрев на мелспектрограмму, сможет _визуально_ проклассифицировать объект.\n\nЭто позволяет свести задачу классификации аудиозаписей к задаче классификации картинок.\n\nРеализуем такую сверточную сеть:\n\n* 2x (Conv2d 3x3 @ 16, BatchNorm2d, ReLU)\n* MaxPoll 2x2\n* 2x (Conv2d 3x3 @ 32, BatchNorm2d, ReLU)\n* MaxPoll 2x2\n* 2x (Conv2d 3x3 @ 64, BatchNorm2d, ReLU)\n* MaxPoll 2x2\n* (Conv2d 3x3 @ 128, BatchNorm2d, ReLU)\n* (Conv2d 2x2 @ 128, BatchNorm2d, ReLU)\n* Global MaxPoll\n* Fully Connected 128, ReLU\n* Fully Connected 10\n\nСовет: подобная архитектура была реализована в [**PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition**](https://arxiv.org/pdf/1912.10211.pdf), можете использовать эту статью как референс.\n\n\n**За реализацию архитектуры дается 1.5 балла.**","metadata":{"id":"Od6HOpq8FXgH"}},{"cell_type":"code","source":"class CNN10(nn.Module):\n    def __init__(self, num_classes=10, hidden=16):\n        super().__init__()\n\n        ### YOUR CODE IS HERE ######\n#         Реализуем такую сверточную сеть:\n\n# (Conv2d 3x3 @ 128, BatchNorm2d, ReLU)\n# (Conv2d 2x2 @ 128, BatchNorm2d, ReLU)\n# Global MaxPoll\n# Fully Connected 128, ReLU\n# Fully Connected 10\n        self.cnn_backbone = nn.Sequential(\n            nn.Conv2d(1, 16, 3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, 3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, 3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, 2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n        )\n\n        self.final_mlp = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        ### THE END OF YOUR CODE ###\n\n    def forward(self, x, lens):\n        z = self.cnn_backbone(x[:, None, :, :])\n        z = torch.nn.functional.max_pool2d(z, kernel_size=z.size()[2:])[:, :, 0, 0]\n        return self.final_mlp(z)","metadata":{"id":"3BSb2SWxFXgI","execution":{"iopub.status.busy":"2022-04-01T20:05:12.858724Z","iopub.execute_input":"2022-04-01T20:05:12.859004Z","iopub.status.idle":"2022-04-01T20:05:12.870381Z","shell.execute_reply.started":"2022-04-01T20:05:12.858969Z","shell.execute_reply":"2022-04-01T20:05:12.86964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = CNN10()\ncnn.to(device);\n\noptim = torch.optim.Adam(cnn.parameters(), lr=3e-4)","metadata":{"id":"9mmkvk5nFXgK","execution":{"iopub.status.busy":"2022-04-01T20:05:13.620028Z","iopub.execute_input":"2022-04-01T20:05:13.620776Z","iopub.status.idle":"2022-04-01T20:05:13.634452Z","shell.execute_reply.started":"2022-04-01T20:05:13.620727Z","shell.execute_reply":"2022-04-01T20:05:13.633746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram,\n                 num_epochs=10)","metadata":{"id":"_-z39AbO8_3Z","outputId":"628d3ad4-46d8-4357-da34-9bb06601d8e4","execution":{"iopub.status.busy":"2022-04-01T20:09:27.764355Z","iopub.execute_input":"2022-04-01T20:09:27.764616Z","iopub.status.idle":"2022-04-01T20:13:06.413786Z","shell.execute_reply.started":"2022-04-01T20:09:27.764586Z","shell.execute_reply":"2022-04-01T20:13:06.413011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Задание: для получения 0.5 балла сделайте подбор гиперпараметров и добейтесь accuracy модели выше 0.85 на валидационном датасете.**","metadata":{"id":"lM-B_VQZ-dJX"}},{"cell_type":"code","source":"plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"id":"PVbI7IAcQNjX","outputId":"03292c60-f6f6-464b-fe3d-02daa9253fdc","execution":{"iopub.status.busy":"2022-04-01T20:13:06.415591Z","iopub.execute_input":"2022-04-01T20:13:06.415848Z","iopub.status.idle":"2022-04-01T20:13:10.355362Z","shell.execute_reply.started":"2022-04-01T20:13:06.415812Z","shell.execute_reply":"2022-04-01T20:13:10.354421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 6. Аугментация SpecAugment (2 балла)\n\nОбычно датасеты с аудиозаписями довольно малы. Наш датасет тому пример - всего 4500 объектов в обучающей выборке. Обучение глубокий сетей с большим кол-вом параметров на таких датасетах часто ведет к переобучению и проседанию метрик на валидационном и тестовом датасетах.\n\nДля борьбы с переобучением имеет смысл использовать аугментацию данных. Для мелспектрограмм была придумана аугментация под названием SpecAugment.\n\nСмысл её очень прост - зануление спектрограммы по временным промежуткам и по мел-частотам:\n1. Выбираются несколько временных промежутков ${[t^1_i, t^2_i]}$ и заполняют спектрограмму $s[t^1_i : t^2_i, :]$ значением $v$.\n\n2. Выбираются несколько промежутков мелчастот ${[m^1_i, m^2_i]}$ и заполняют спектрограмму $s[:, m^1_i : m^2_i]$ значением $v$.\n\nВ качестве значения $v$ выбирают:\n1. `'mean'`: среднее по спектрограмме\n2. `'min'`: минимум по спектрограмме\n3. `'max'`: максимум по спектрограмме\n5. `v`: некоторая константа\n\nСовет: описание аугментации можно найти здесь: [link](https://neurohive.io/ru/novosti/specaugment-novyj-metod-augmentacii-audiodannyh-ot-google-ai/), можете использовать эту ссылку как референс.\n\n![specaugment](https://neurohive.io/wp-content/uploads/2019/04/image6.png)\n\nВ этом задании Вам предлагается реализовать аугментацию SpecAugment.\n\n**За реализацию аугментации дается 1.5 балла.**\n","metadata":{"id":"cu9fpMOikY6L"}},{"cell_type":"code","source":"import random\n\n\nclass SpectAugment:\n    def __init__(\n        self,\n        filling_value = \"mean\",\n        n_freq_masks = 2,\n        n_time_masks = 2,\n        max_freq = 10,\n        max_time = 50,\n    ):\n\n        self.filling_value = filling_value\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n        self.max_freq = max_freq\n        self.max_time = max_time\n\n    def __call__(self, spect, lens):\n        ### YOUR CODE IS HERE ######\n        \n        ### THE END OF YOUR CODE ###","metadata":{"id":"lmfkTTrfFXgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# применим аугментацию к данным\nfor batch in train_dataloader:\n    break\n\nx = batch[\"x\"].to(device)\nlens = batch[\"len\"].to(device)\nx_logmel, lens = compute_log_melspectrogram_reference(x, lens, sr=train_dataset.sr, device=device)\nx_logmel_augmented, lens = SpectAugment()(x_logmel, lens)\n\n# нарисуем спектрограмму до и после аугментации\nplt.figure(figsize=(20, 5))\nplt.subplot(2, 1, 1)\nplt.title(\"Original log MelSpectrogram\")\nplt.axis(\"off\")\nplt.imshow(x_logmel[0].cpu().numpy())\n\nplt.subplot(2, 1, 2)\nplt.title(\"Augmented log MelSpectrogram\")\nplt.axis(\"off\")\nplt.imshow(x_logmel_augmented[0].cpu().numpy())\n\nplt.show()","metadata":{"id":"GXaJi2nfqE4J","outputId":"ce1d6da7-bfb8-4caa-9697-dc5f0332a679"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = CNN10()\ncnn.to(device);\n\noptim = torch.optim.Adam(cnn.parameters(), lr=3e-4)","metadata":{"id":"XdohpTw5l6Lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучим модель на данных с аугментациями\ntrain_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram,\n                 augmentation=SpectAugment(),\n                 num_epochs=20)","metadata":{"id":"59YvirCHm_ye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Задание: для получения 0.5 балла сделайте подбор параметров аугментации и добейтесь accuracy модели выше 0.9 на валидационном датасете.**","metadata":{"id":"iI_0no5Z_OZv"}},{"cell_type":"code","source":"plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"id":"F7kgOAn-nHSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 вариант\n\nhttps://music-classification.github.io/tutorial/part3_supervised/tutorial.html","metadata":{}},{"cell_type":"code","source":"!pip install torchaudio_augmentations","metadata":{"execution":{"iopub.status.busy":"2023-04-18T17:14:02.281243Z","iopub.execute_input":"2023-04-18T17:14:02.281532Z","iopub.status.idle":"2023-04-18T17:14:13.711201Z","shell.execute_reply.started":"2023-04-18T17:14:02.281500Z","shell.execute_reply":"2023-04-18T17:14:13.710327Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchaudio_augmentations\n  Downloading torchaudio_augmentations-0.2.4-py3-none-any.whl (12 kB)\nCollecting wavaugment\n  Downloading wavaugment-0.2-py3-none-any.whl (5.4 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchaudio_augmentations) (1.9.1)\nCollecting julius\n  Downloading julius-0.2.7.tar.gz (59 kB)\n     |████████████████████████████████| 59 kB 6.5 MB/s             \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting torch-pitch-shift\n  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (from torchaudio_augmentations) (0.9.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchaudio_augmentations) (1.20.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchaudio_augmentations) (4.1.1)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from torch-pitch-shift->torchaudio_augmentations) (21.3)\nCollecting primePy>=1.3\n  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->torch-pitch-shift->torchaudio_augmentations) (3.0.6)\nBuilding wheels for collected packages: julius\n  Building wheel for julius (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21895 sha256=69b4465e70424c54a4ae4135ee4ce3934b11a796775f0bffb6aebc3ca832d786\n  Stored in directory: /root/.cache/pip/wheels/44/52/2c/7dd069f82c7f905f40b190a8039ec2a17fdd4bb009c57c6664\nSuccessfully built julius\nInstalling collected packages: primePy, wavaugment, torch-pitch-shift, julius, torchaudio-augmentations\nSuccessfully installed julius-0.2.7 primePy-1.3 torch-pitch-shift-1.2.4 torchaudio-augmentations-0.2.4 wavaugment-0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport soundfile as sf\nfrom torch.utils import data\nfrom torchaudio_augmentations import (\n    RandomResizedCrop,\n    RandomApply,\n    PolarityInversion,\n    Noise,\n    Gain,\n    HighLowPass,\n    Delay,\n    PitchShift,\n    Reverb,\n    Compose,\n)\n\n\nGTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n\n\nclass GTZANDataset(data.Dataset):\n    def __init__(self, data_path, split, num_samples, num_chunks, is_augmentation):\n        self.data_path =  data_path if data_path else ''\n        self.split = split\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.is_augmentation = is_augmentation\n        self.genres = GTZAN_GENRES\n        self._get_song_list()\n        if is_augmentation:\n            self._get_augmentations()\n\n    def _get_song_list(self):\n        list_filename = os.path.join(self.data_path, '%s_filtered.txt' % self.split)\n        with open(list_filename) as f:\n            lines = f.readlines()\n        self.song_list = [line.strip() for line in lines]\n\n    def _get_augmentations(self):\n        transforms = [\n            RandomResizedCrop(n_samples=self.num_samples),\n            RandomApply([PolarityInversion()], p=0.8),\n            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n            RandomApply([Gain()], p=0.2),\n            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n            RandomApply([Delay(sample_rate=22050)], p=0.5),\n            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n        ]\n        self.augmentation = Compose(transforms=transforms)\n\n    def _adjust_audio_length(self, wav):\n        if self.split == 'train':\n            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n            wav = wav[random_index : random_index + self.num_samples]\n        else:\n            hop = (len(wav) - self.num_samples) // self.num_chunks\n            wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)])\n        return wav\n\n    def __getitem__(self, index):\n        line = self.song_list[index]\n\n        # get genre\n        genre_name = line.split('/')[0]\n        genre_index = self.genres.index(genre_name)\n\n        # get audio\n        audio_filename = os.path.join(self.data_path, 'genres', line)\n        wav, fs = sf.read(audio_filename)\n\n        # adjust audio length\n        wav = self._adjust_audio_length(wav).astype('float32')\n\n        # data augmentation\n        if self.is_augmentation:\n            wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0)).squeeze(0).numpy()\n\n        return wav, genre_index\n\n    def __len__(self):\n        return len(self.song_list)\n\ndef get_dataloader(data_path=None, \n                   split='train', \n                   num_samples=22050 * 29, \n                   num_chunks=1, \n                   batch_size=16, \n                   num_workers=0, \n                   is_augmentation=False):\n    is_shuffle = True if (split == 'train') else False\n    batch_size = batch_size if (split == 'train') else (batch_size // num_chunks)\n    data_loader = data.DataLoader(dataset=GTZANDataset(data_path, \n                                                       split, \n                                                       num_samples, \n                                                       num_chunks, \n                                                       is_augmentation),\n                                  batch_size=batch_size,\n                                  shuffle=is_shuffle,\n                                  drop_last=False,\n                                  num_workers=num_workers)\n    return data_loader\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T17:15:16.473598Z","iopub.execute_input":"2023-04-18T17:15:16.473902Z","iopub.status.idle":"2023-04-18T17:15:16.573329Z","shell.execute_reply.started":"2023-04-18T17:15:16.473868Z","shell.execute_reply":"2023-04-18T17:15:16.572355Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/4018041545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from torchaudio_augmentations import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mRandomResizedCrop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mRandomApply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio_augmentations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_low_pass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHighLowPass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpitch_shift\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPitchShift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_inversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolarityInversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_resized_crop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomResizedCrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio_augmentations/augmentations/pitch_shift.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/augment/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .effects import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mEffectChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mshutdown_sox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/augment/effects.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_effect_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_effect_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEffectChain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_effect_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_effect_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/_internal/module_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{func.__module__}.{func.__name__} requires sox'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: torchaudio.sox_effects.sox_effects.effect_names requires sox"],"ename":"RuntimeError","evalue":"torchaudio.sox_effects.sox_effects.effect_names requires sox","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:49:28.135614Z","iopub.execute_input":"2023-04-19T13:49:28.135888Z","iopub.status.idle":"2023-04-19T13:49:28.140562Z","shell.execute_reply.started":"2023-04-19T13:49:28.135856Z","shell.execute_reply":"2023-04-19T13:49:28.139860Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(names_weights_height.values())","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:52:04.415919Z","iopub.execute_input":"2023-04-19T13:52:04.416197Z","iopub.status.idle":"2023-04-19T13:52:04.423090Z","shell.execute_reply.started":"2023-04-19T13:52:04.416165Z","shell.execute_reply":"2023-04-19T13:52:04.420477Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:52:09.468086Z","iopub.execute_input":"2023-04-19T13:52:09.468655Z","iopub.status.idle":"2023-04-19T13:52:09.480825Z","shell.execute_reply.started":"2023-04-19T13:52:09.468616Z","shell.execute_reply":"2023-04-19T13:52:09.479619Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"     0   1\n0  184  83\n1  182  78\n2  188  78\n3  191  87\n4  192  88\n5  186  80\n6  187  71\n7  177  70","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>184</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>182</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>188</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>191</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>192</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>186</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>187</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>177</td>\n      <td>70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"plt.scatter(df[0], df[1])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:53:26.658080Z","iopub.execute_input":"2023-04-19T13:53:26.658370Z","iopub.status.idle":"2023-04-19T13:53:26.886664Z","shell.execute_reply.started":"2023-04-19T13:53:26.658336Z","shell.execute_reply":"2023-04-19T13:53:26.885973Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7f7dc9e35b90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"df[0].plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:58:14.185145Z","iopub.execute_input":"2023-04-19T13:58:14.185713Z","iopub.status.idle":"2023-04-19T13:58:14.398340Z","shell.execute_reply.started":"2023-04-19T13:58:14.185676Z","shell.execute_reply":"2023-04-19T13:58:14.397641Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"df[1].plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T14:01:41.238519Z","iopub.execute_input":"2023-04-19T14:01:41.238791Z","iopub.status.idle":"2023-04-19T14:01:41.432784Z","shell.execute_reply.started":"2023-04-19T14:01:41.238760Z","shell.execute_reply":"2023-04-19T14:01:41.432109Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}